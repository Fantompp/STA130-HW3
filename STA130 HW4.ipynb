{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# STA130 HW04\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 1. The \"Pre-lecture\" video (above) mentioned the \"standard error of the mean\" as being the \"standard deviation\" of the distribution bootstrapped means. What is the difference between the \"standard error of the mean\" and the \"standard deviation\" of the original data? What distinct ideas do each of these capture? Explain this concisely in your own words.\n    The standard deviation is the spread of the actual data we collected, whereas the standard error is the spread of the boot-strapped sample means. \n\n    The standard error says \"if we drew another sample from the data, this is how far we expect the mean of that sample to be from our original sample mean.\" \n\n    The standard deviation says \"if we drew another data point from the population, this is how far we expect that data point to be from our original sample mean.\"",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 6. Provide written answers explaining the answers to the following questions in an informal manner of a conversation with a friend with little experience with statistics.\n\n1. What is the process of bootstrapping?\n\n    So, you've got this sample, which you picked from the population. Let's say you want to find the average squeekiness of all rubber ducks in Toronto.\n\n   Your population is the set of all rubber ducks in Toronto. Unfortunately, due to budget constraints, you can't squeeze every rubber duck in Toronto, no matter how much you want to. Because it's like animal abuse or something.\n\n   Thus, you grab 1000 rubber ducks from around the city, record the squeekiness each one, and write it on the bottom of each duck.\n\n   You could very easily calculate the average squeekiness now, and you probably will. But once again, there are a *lot* of rubber ducks in Toronto. How do we know if our average squeekiness is accurate?\n\n   The truth is, we don't, and we never will. BUT, we can make an estimate. Simply toss all 1000 ducks in a pool, set up a little robot which will scan 100 ducks randomly, and have it do that 10000 times (it might take a while). What this does is essentially trade the size of the sample for a greater number of samples. The average of these duck scans will ultimately approach the first average we calculated, but what this buys us is the distribution of averages.\n\n   If our sample is representative of the population (fingers crossed), then by drawing a bunch of smaller samples from the sample, the distribution of averages we see in the smaller samples, should ultimately be very similar to if we really did draw a bunch of samples from the population. \n\n3. What is the main purpose of bootstrapping?\n\n    So now you have this distribution of averages. This essentially tells you how certain you are about your result. Instead of having a single number for average squeekiness, you have a distribution (whole bunch of numbers).\n\n   Since we're assuming our sample looks pretty similar to the population, you can easily take the middle of that distribution, and say \"well look, if you took a bunch of samples from the population, 95% of the times the average will lie between these two numbers.\"\n\n   So this way, instead of saying \"we measured the average squeekiness to be 7.355\", you could say \"we're 95% sure that the average squeekiness lies somewhere between 7.124 and 7.423.\" Like those fancy science people do. \n\n5. If you had a (hypothesized) guess about what the average of a population was, and you had a sample of size n from that population, how could you use bootstrapping to assess whether or not your (hypothesized) guess might be plausible?\n\n    You could take some bootstrapped samples (robot scanny beet boots) from our sample of size n. This, as described earlier, gives us a distribution of *sample* averages. From there, you can just look at how far away your guess is from the middle of the pack.\n\n   If 90% of the bootstrapped samples had a higher average than your guess, you might decide that your guess was a bit low. But if your guess was right in the middle, and 50% of the times a bootstrapped sample would have a higher average, 50% of the times lower, then that's probably a decent guess.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 8. Complete the following assignment.\nVaccine Data Analysis Assignment\n\n#### Overview\n\nThe company AliTech has created a new vaccine that aims to improve the health of the people who take it. Your job is to use what you have learned in the course to give evidence for whether or not the vaccine is effective.\n\nData AliTech has released the following data.\n\ncsv\nPatientID,Age,Gender,InitialHealthScore,FinalHealthScore\n1,45,M,84,86\n2,34,F,78,86\n3,29,M,83,80\n4,52,F,81,86\n5,37,M,81,84\n6,41,F,80,86\n7,33,M,79,86\n8,48,F,85,82\n9,26,M,76,83\n10,39,F,83,84\n\n**Deliverables** While you can choose how to approach this project, the most obvious path would be to use bootstrapping, follow the analysis presented in the \"Pre-lecture\" HW video (above). Nonetheless, we are primarily interested in evaluating your report relative to the following deliverables.\n\nA visual presentation giving some initial insight into the comparison of interest.\nA quantitative analysis of the data and an explanation of the method and purpose of this method.\nA conclusion regarding a null hypothesis of \"no effect\" after analyzing the data with your methodology.\nThe clarity of your documentation, code, and written report.\nConsider organizing your report within the following outline template.\n\n- Problem Introduction\n- An explaination of the meaning of a Null Hypothesis of \"no effect\" in this context\n- Data Visualization (motivating and illustrating the comparison of interest)\n- Quantitative Analysis\n- Methodology Code and Explanations\n- Supporting Visualizations\n- Findings and Discussion\n- Conclusion regarding a Null Hypothesis of \"no effect\"\n- Further Considerations\n\n#### Further Instructions\nWhen using random functions, you should make your analysis reproducible by using the np.random.seed() function\nCreate a CSV file and read that file in with your code, but do not include the CSV file along with your submission",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 8. See attached Colab for question 8. The python code wouldn't run for me here. ",
      "metadata": {}
    }
  ]
}